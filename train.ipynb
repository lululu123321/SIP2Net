{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from skimage import io, transform\n",
    "from skimage.io import imread\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors as colrs\n",
    "from torch.utils.data import Dataset, DataLoader, ConcatDataset\n",
    "from torchvision import transforms, utils, datasets, models\n",
    "import sys \n",
    "sys.path.append(\"..\") \n",
    "from load_data_task3_aug import DataGenerator\n",
    "import cv2\n",
    "import copy \n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import os  \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3750 150 cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "\n",
    "input_path =  \"/ICASSP2025_Dataset/Inputs/Task_3_ICASSP/\" \n",
    "output_path = \"/ICASSP2025_Dataset/Outputs/Task_3_ICASSP/\" \n",
    "csv_path = \"/ICASSP2025_Dataset/\"\n",
    "\n",
    "Input_list_file_names_train = []\n",
    "for b in range(1, 26):  # 25 for train 1 for eval \n",
    "    for ant in range(1, 2):\n",
    "        if ant == 1:\n",
    "            s_range = range(50)\n",
    "        else:\n",
    "            s_range = range(80)\n",
    "        for s in s_range:\n",
    "            for f in {1, 2, 3}:\n",
    "                Input_list_file_names_train.append(\"B\" + str(b) + \"_Ant\" + str(ant) + \"_f\" + str(f) + \"_S\" + str(s))\n",
    "\n",
    "Input_list_file_names_eval = []\n",
    "for b in range(25, 26):  # 25 for train 1 for eval \n",
    "    for ant in range(1, 2):\n",
    "        if ant == 1:\n",
    "            s_range = range(50)\n",
    "        else:\n",
    "            s_range = range(80)\n",
    "        for s in s_range:\n",
    "            for f in {1, 2, 3}:\n",
    "                Input_list_file_names_eval.append(\"B\" + str(b) + \"_Ant\" + str(ant) + \"_f\" + str(f) + \"_S\" + str(s))\n",
    "\n",
    "\n",
    "\n",
    "Input_list_IDs_train =  np.arange(0, len(Input_list_file_names_train), 1, dtype=int)\n",
    "\n",
    "train_generator = DataGenerator (device, Input_list_IDs_train, Input_list_file_names_train, input_path, output_path,\n",
    "                 dim_X = (256,256), dim_y = (256,256),  csv_path=csv_path, random_augment = 1,\n",
    "                 shuffle=True)\n",
    "\n",
    "Input_list_IDs_eval =  np.arange(0, len(Input_list_file_names_eval), 1, dtype=int)\n",
    "eval_generator = DataGenerator (device, Input_list_IDs_eval, Input_list_file_names_eval, input_path, output_path,\n",
    "                 dim_X = (256,256), dim_y = (256,256),  csv_path=csv_path, random_augment = 1,\n",
    "                 shuffle=True)\n",
    "\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_generator, batch_size=16, shuffle=True),\n",
    "    'val': DataLoader(eval_generator, batch_size=16, shuffle=True)\n",
    "}\n",
    "print(len(train_generator), len(eval_generator), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Try the best aug method based on kaggle result\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# input_path =  \"/ICASSP2025_Dataset/Inputs/Task_3_ICASSP/\" \n",
    "# output_path = \"/ICASSP2025_Dataset/Outputs/Task_3_ICASSP/\" \n",
    "# csv_path = \"/ICASSP2025_Dataset/\"\n",
    "\n",
    "# Input_list_file_names_train = []\n",
    "# for b in range(1, 26):  # 25 for train 1 for eval \n",
    "#     for ant in range(1, 2):\n",
    "#         if ant == 1:\n",
    "#             s_range = range(50)\n",
    "#         else:\n",
    "#             s_range = range(80)\n",
    "#         for s in s_range:\n",
    "#             for f in {1, 2, 3}:\n",
    "#                 Input_list_file_names_train.append(\"B\" + str(b) + \"_Ant\" + str(ant) + \"_f\" + str(f) + \"_S\" + str(s))\n",
    "\n",
    "# Input_list_file_names_eval = []\n",
    "# for b in range(25, 26):  # 25 for train 1 for eval \n",
    "#     for ant in range(1, 2):\n",
    "#         if ant == 1:\n",
    "#             s_range = range(50)\n",
    "#         else:\n",
    "#             s_range = range(80)\n",
    "#         for s in s_range:\n",
    "#             for f in {1, 2, 3}:\n",
    "#                 Input_list_file_names_eval.append(\"B\" + str(b) + \"_Ant\" + str(ant) + \"_f\" + str(f) + \"_S\" + str(s))\n",
    "\n",
    "# augment_modes = [\"original\", \"flip_y\", \"flip_x\", \"rotate_90\", \"rotate_180\", \"rotate_270\", \"flip_x_rotate_90\", \"flip_y_rotate_90\"]\n",
    "\n",
    "# Input_list_IDs_train =  np.arange(0, len(Input_list_file_names_train), 1, dtype=int)\n",
    "# Input_list_IDs_eval =  np.arange(0, len(Input_list_file_names_eval), 1, dtype=int)\n",
    "\n",
    "# train_dataset = ConcatDataset([\n",
    "#     DataGenerator(device, Input_list_IDs_train, Input_list_file_names_train, input_path, output_path, (256, 256), (256, 256), csv_path, random_augment=0,\n",
    "#                   shuffle=True, augment_mode=mode)\n",
    "#     for mode in augment_modes\n",
    "# ])\n",
    "# eval_dataset = ConcatDataset([\n",
    "#     DataGenerator(device, Input_list_IDs_eval, Input_list_file_names_eval, input_path, output_path, (256, 256), (256, 256), csv_path, random_augment=0,\n",
    "#                   shuffle=True, augment_mode=mode)\n",
    "#     for mode in augment_modes\n",
    "# ])\n",
    "\n",
    "# dataloaders = {\n",
    "#     'train': DataLoader(train_dataset, batch_size=16, shuffle=True, drop_last=True),\n",
    "#     'val': DataLoader(eval_dataset, batch_size=16, shuffle=True, drop_last=True)\n",
    "# }\n",
    "# print(len(train_dataset), len(eval_dataset), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "from torchmetrics import StructuralSimilarityIndexMeasure as SSIM\n",
    "from torchmetrics.functional.image import image_gradients\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm \n",
    "\n",
    "def calc_loss_mix_ssim_gradient(pred, target, metrics):\n",
    "    target = target.unsqueeze(1)\n",
    "    target_dy, target_dx = image_gradients(target)\n",
    "    pred_dy, pred_dx = image_gradients(pred)\n",
    "    gdl = (target_dy - pred_dy).pow(2) + (target_dx - pred_dx).pow(2)\n",
    "    loss1 = gdl.sum() / pred.numel()\n",
    "\n",
    "    ssim = SSIM(data_range=255.0).to(device)\n",
    "    loss2 = 1 - ssim(pred, target)\n",
    "\n",
    "    l1_loss = nn.L1Loss()\n",
    "    loss_l1 = l1_loss(pred, target)\n",
    "\n",
    "    mse = nn.MSELoss()\n",
    "    loss_mse = mse(pred, target)\n",
    "\n",
    "    loss = 100 * loss2 + loss_l1\n",
    "    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n",
    "\n",
    "    metrics['L1 loss'] += loss_l1.data.cpu().numpy() * target.size(0)\n",
    "    metrics['1-ssim loss'] += loss2.data.cpu().numpy() * target.size(0)\n",
    "    metrics['GDL loss'] += loss1.data.cpu().numpy() * target.size(0)\n",
    "    metrics['MSE loss'] += loss_mse.data.cpu().numpy() * target.size(0)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def calc_discriminator_loss(real_output, fake_output, metrics):\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    real_labels = torch.ones(real_output.size(0), 1).to(device)\n",
    "    fake_labels = torch.zeros(fake_output.size(0), 1).to(device)\n",
    "    \n",
    "    real_loss = criterion(real_output, real_labels)\n",
    "    fake_loss = criterion(fake_output, fake_labels) \n",
    "\n",
    "    return real_loss + fake_loss\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):\n",
    "    outputs1 = []\n",
    "    for k in metrics.keys():\n",
    "        outputs1.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs1)))\n",
    "\n",
    "def train_gan(generator, discriminator, g_optimizer, d_optimizer, device, num_epochs=10):\n",
    "    best_model_wts = copy.deepcopy(generator.state_dict())\n",
    "    best_loss = 1e10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        since = time.time()\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                generator.train() \n",
    "                discriminator.train()  \n",
    "            else:\n",
    "                generator.eval()   \n",
    "                discriminator.eval()  \n",
    "\n",
    "            metrics = defaultdict(float)\n",
    "            epoch_samples = 0\n",
    "\n",
    "            with tqdm(dataloaders[phase], unit=\"batch\", disable=False) as tepoch:\n",
    "                for inputs, targets in tepoch:\n",
    "                    tepoch.set_description(f\"Epoch {epoch + 1}/{num_epochs} ({phase})\")\n",
    "\n",
    "                    inputs = inputs.permute(0, 3, 1, 2)  # bx3x256x256\n",
    "                    inputs = inputs.float().to(device)\n",
    "                    targets = targets.float().to(device)\n",
    "\n",
    "                    d_optimizer.zero_grad()\n",
    "                    \n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        fake_images = generator(inputs).detach() \n",
    "                        \n",
    "                        combined_targets = torch.cat((inputs, targets.unsqueeze(1)), dim=1)\n",
    "                        combined_fake = torch.cat((inputs, fake_images), dim=1)\n",
    "\n",
    "                        real_output = discriminator(combined_targets)\n",
    "                        fake_output = discriminator(combined_fake)\n",
    "\n",
    "                        d_loss = calc_discriminator_loss(real_output, fake_output, metrics)\n",
    "                        if phase == 'train':\n",
    "                            d_loss.backward()\n",
    "                            d_optimizer.step()\n",
    "                    \n",
    "                    g_optimizer.zero_grad()\n",
    "                    \n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        fake_images = generator(inputs)\n",
    "                        combined_fake = torch.cat((inputs, fake_images), dim=1)\n",
    "                        fake_output = discriminator(combined_fake)\n",
    "                        \n",
    "                        # g_loss = -torch.mean(fake_output) + calc_loss_mix_ssim_gradient(fake_images, targets, metrics) # find best kaggle result\n",
    "                        g_loss = calc_loss_mix_ssim_gradient(fake_images, targets, metrics)\n",
    "                        \n",
    "                        if phase == 'train':\n",
    "                            g_loss.backward()\n",
    "                            g_optimizer.step()\n",
    "\n",
    "                    epoch_samples += inputs.size(0)\n",
    "\n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "            if phase == 'val':\n",
    "                # model_save_path = f\"/data1/lihan_data/task3/20241120_ft/t3model_epoch{epoch + 1}_valloss{epoch_loss:.4f}.pt\"\n",
    "                # torch.save(generator.state_dict(), model_save_path)\n",
    "                # print(f\"Model saved at {model_save_path}\")\n",
    "                \n",
    "                if epoch_loss < best_loss:\n",
    "                    print(\"saving best model\")\n",
    "                    best_loss = epoch_loss\n",
    "                    best_model_wts = copy.deepcopy(generator.state_dict())\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "    \n",
    "    generator.load_state_dict(best_model_wts)\n",
    "    return generator, discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "Epoch 0/49\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 (train):   1%|          | 2/235 [00:07<14:52,  3.83s/batch]"
     ]
    }
   ],
   "source": [
    "from model.SIP2Net import SIP2Net\n",
    "\n",
    "model_G = SIP2Net(\n",
    "    n_blocks=[3, 3, 9, 3],  \n",
    "    atrous_rates=[2, 4, 6],  \n",
    "    multi_grids=[1, 2, 4],  \n",
    "    output_stride=8,        \n",
    ")\n",
    "model_G.to(device)\n",
    "\n",
    "from model.dis_model import Discriminator\n",
    "model_D = Discriminator()\n",
    "model_D.to(device)\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "# from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "\n",
    "torch.backends.cudnn.enabled\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "optimizer_G = optim.AdamW(model_G.parameters(), lr=0.00005, weight_decay=1e-5)\n",
    "optimizer_D = optim.AdamW(model_D.parameters(), lr=0.00005, weight_decay=1e-5)\n",
    "\n",
    "model = train_gan(generator=model_G, discriminator=model_D, g_optimizer=optimizer_G, d_optimizer=optimizer_D, device=device, num_epochs=50)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "icassp2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
